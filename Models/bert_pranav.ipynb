{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "poaHMqRaEo0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install errant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE5M9nwyp1sK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9e2b693-9f32-4b46-80d5-0805aad7a6db"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1_CDVJuLjD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4b65d713-3a59-4246-92c5-833d2aa102cc"
      },
      "source": [
        "!git clone https://github.com/samueljamesbell/m2-correct"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'm2-correct'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Total 26 (delta 0), reused 0 (delta 0), pack-reused 26\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtLYFh4iEpp9",
        "colab_type": "text"
      },
      "source": [
        "First task: we could take the word that is the max suggest replacement AND in the same word set\n",
        "- mask each token one at a time\n",
        "- determine which one is the error based on the confidence\n",
        "- make the replacement\n",
        "- check if we got it right\n",
        "\n",
        "Second task: \n",
        "\n",
        "- same thing but doesn't know, so we replace as many across a certain threshold\n",
        "- shows how good bert is correcting for an overall replacement for a sentence (more than single edit?)\n",
        "\n",
        "- same thing but we do it for each word, above a maximum confidence (above 30? or 40?)\n",
        "\n",
        "What we want overall: Handle data from our original data file, flattened. Takes wrong sentence, provides suggestions, makes the correction, and compares them to the correct sentence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvBRhkaYtYsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uU_5paMtHmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import pipeline, BertTokenizer, BertModel, BertForMaskedLM\n",
        "import sys, string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJIAtkmWtwAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "import torch\n",
        "from operator import itemgetter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJtqFvxYuNLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fstr(sentence):\n",
        "    return eval(f\"f'{sentence}'\")\n",
        "\n",
        "def mask(sentence, masked_index):\n",
        "    return sentence[:masked_index] + ['[MASK]'] + sentence[masked_index+1:]\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"bert-base-uncased\",\n",
        "    tokenizer=\"bert-base-uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_vEA94QzVbo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "59d28444-cfee-465b-a4b9-d89fd55559ab"
      },
      "source": [
        "print(fill_mask(f\"HuggingFace is creating a [MASK] that the community uses to solve NLP tasks.\"))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'sequence': '[CLS] huggingface is creating a tool that the community uses to solve nlp tasks. [SEP]', 'score': 0.3885316550731659, 'token': 6994}, {'sequence': '[CLS] huggingface is creating a framework that the community uses to solve nlp tasks. [SEP]', 'score': 0.0572054386138916, 'token': 7705}, {'sequence': '[CLS] huggingface is creating a database that the community uses to solve nlp tasks. [SEP]', 'score': 0.048818349838256836, 'token': 7809}, {'sequence': '[CLS] huggingface is creating a program that the community uses to solve nlp tasks. [SEP]', 'score': 0.04593093693256378, 'token': 2565}, {'sequence': '[CLS] huggingface is creating a software that the community uses to solve nlp tasks. [SEP]', 'score': 0.04023849964141846, 'token': 4007}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQB_brFMt4J0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ddad1fd5-1068-4df4-894a-96f4a5f255b2"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdkHg9TdvIQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent1 = 'HuggingFace is creating an tool that the community uses to solve NLP tasks.'\n",
        "sent2 = \"My town is a medium size city with eighty thousand inhabitants .\"\n",
        "sent3 = \"It has a high - density population because its small territory .\"\n",
        "sent4 = \"I recommend visiting the artificial lake in the certer of the city which is surrounded by a park .\"\n",
        "sent5 = \"The quality of the products and services is quite good , because there is huge competition . However recommend you be careful about some fakes or cheats .\"\n",
        "\n",
        "data = {'sentence': sent5, 'correction': '', 'index': []}\n",
        "tokenized_sentence = tokenizer.tokenize(data['sentence'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiumXXTCy-i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k_pred = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_LRuMJBvL_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_predict_token(tokenized_sentence):\n",
        "    best_predictions = []\n",
        "    orignal_sentence = tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized_sentence))\n",
        "\n",
        "    for i, word in enumerate(tokenized_sentence):\n",
        "        if tokenized_sentence[i] in string.punctuation: #avoid calculating punctuation which will likely have the highest result\n",
        "            continue;\n",
        "\n",
        "        masked_sentence = tokenizer.convert_tokens_to_string(mask(tokenized_sentence, i)).replace(' ##', '')\n",
        "\n",
        "        #Note for later: make sure these partial tokens (##) won't cause any problems\n",
        "\n",
        "        #ranked 1 prediction\n",
        "        prediction = fill_mask(masked_sentence)[0]\n",
        "\n",
        "        #removing special tokens\n",
        "        tokenized_prediction = tokenizer.tokenize(prediction['sequence']) #tokenizing prediction sentence                                      \n",
        "        no_special_tokens_sentence = tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized_prediction), skip_special_tokens=True) #actual sentence\n",
        "\n",
        "        if (orignal_sentence != no_special_tokens_sentence):\n",
        "            prediction['no_special_token_sentence'] = no_special_tokens_sentence\n",
        "            best_predictions.append(prediction)\n",
        "\n",
        "        #print(word, prediction)\n",
        "\n",
        "    sorted_best_predictions = sorted(best_predictions, key=itemgetter('score'), reverse=True) \n",
        "    \n",
        "    highest_confidence_corrections = []\n",
        "    for k in range(min(top_k_pred, len(orignal_sentence))):\n",
        "        highest_confidence_corrections.append(sorted_best_predictions[k])\n",
        "\n",
        "    top_k_tokens = []\n",
        "    for ele in highest_confidence_corrections:\n",
        "        top_k_tokens.append(tokenizer.convert_ids_to_tokens(ele['token']))\n",
        "\n",
        "    # for ele in sorted_best_predictions:\n",
        "    #     print(ele)\n",
        "\n",
        "    #toke sentence with highest confidence correction without special tokens\n",
        "    return(top_k_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Wmt8exA5A9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a898be5-9f74-47f1-ea44-63f9588d37af"
      },
      "source": [
        "mask_predict_token(tokenized_sentence)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',', 'and', 'fake', 'cheat', 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVL2Q-3eL1RF",
        "colab_type": "text"
      },
      "source": [
        "-Now using output3.txt from the AP-GEC github.\n",
        "\n",
        "-Evaluating sentences from the edits using the m2-correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh8DcbeCul7p",
        "colab_type": "text"
      },
      "source": [
        "Note: Does not identify spelling errors - so at the moment I am skipping them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYsIzN_2Qv5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_file = open('output3.txt')\n",
        "preprocessed_file_str = preprocessed_file.read()\n",
        "\n",
        "\n",
        "preprocessed_file_split = preprocessed_file_str.split('\\n\\n')\n",
        "sentence_pairs = [ele.split('\\n') for ele in preprocessed_file_split]\n",
        "\n",
        "#remove the 'S' part of the sentences, and extract only the word the edits. Also, only keep replacement edits.\n",
        "new_sentence_pairs = []\n",
        "for ele in sentence_pairs:\n",
        "    try:\n",
        "        temp0 = ele[0][2:]\n",
        "        temp1 = ele[1].split(\"|||\")\n",
        "    except IndexError:\n",
        "        continue\n",
        "    \n",
        "    if (temp1[1][0].lower() != 'r'):\n",
        "        continue\n",
        "\n",
        "    #skipping spelling errors as the model fails to identify them\n",
        "    if (temp1[1][2:].lower() == 'spell'):\n",
        "        continue\n",
        "\n",
        "    temp1 = temp1[2].lower()\n",
        "    new_sentence_pairs.append([temp0, temp1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdm4wTROoGjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e278e648-76d1-4403-9834-384b18ee31cf"
      },
      "source": [
        "#now applying it on the new sentence pairs\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "count = 0\n",
        "correct = 0\n",
        "\n",
        "N = 15\n",
        "i = 0\n",
        "\n",
        "#pbar = tqdm(total=len(new_sentence_pairs))\n",
        "for pair in new_sentence_pairs:\n",
        "    sentence = pair[0]\n",
        "\n",
        "    correct_token = pair[1]\n",
        "    #removing punctuation because BERT predictions don't have punctuation\n",
        "    correct_token = [w.translate(table) for w in correct_token]\n",
        "    correct_token = \"\".join(correct_token)\n",
        "    correct_token = correct_token.strip()\n",
        "\n",
        "    data = {'sentence': sentence, 'correction': '', 'index': []}\n",
        "    tokenized_sentence = tokenizer.tokenize(data['sentence'])\n",
        "    predicted_tokens = mask_predict_token(tokenized_sentence)\n",
        "\n",
        "    print(correct_token)\n",
        "    print(predicted_tokens)\n",
        "\n",
        "    if correct_token in predicted_tokens:\n",
        "        correct += 1\n",
        "        print(\"yes\")\n",
        "    else:\n",
        "        print(\"now\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    count += 1\n",
        "    #pbar.update(1)\n",
        "\n",
        "    i += 1\n",
        "    if (i == N): \n",
        "        break\n",
        "#pbar.close()\n",
        "\n",
        "print(correct)\n",
        "print(count)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sized\n",
            "['the', 'sized', 'town', 'people', 'several']\n",
            "yes\n",
            "\n",
            "although\n",
            "['being', 'that', 'most', 'being', 'area']\n",
            "now\n",
            "\n",
            "of\n",
            "['a', 'of', 'are', '##ls', 'popular']\n",
            "yes\n",
            "\n",
            "is\n",
            "['is', 'part', 'kinds', 'in', 'main']\n",
            "yes\n",
            "\n",
            "kinds\n",
            "['part', 'in', 'kinds', 'main', 'on']\n",
            "yes\n",
            "\n",
            "businesses\n",
            "['part', 'in', 'main', 'on', 'restaurants']\n",
            "now\n",
            "\n",
            "grocers\n",
            "['part', 'in', 'main', 'on', 'restaurants']\n",
            "now\n",
            "\n",
            "is\n",
            "['of', 'is', 'and', 'fake', 'is']\n",
            "yes\n",
            "\n",
            "is\n",
            "['of', 'is', 'and', 'fake', 'any']\n",
            "yes\n",
            "\n",
            "however\n",
            "['of', 'and', 'fake', 'appreciate', 'any']\n",
            "now\n",
            "\n",
            "recommend\n",
            "['and', 'fake', 'of', 'no', 'cheat']\n",
            "now\n",
            "\n",
            "be\n",
            "['of', 'and', 'fake', 'no', 'i']\n",
            "now\n",
            "\n",
            "careful\n",
            "['fake', 'and', 'i', 'no', 'cheat']\n",
            "now\n",
            "\n",
            "of\n",
            "[',', 'and', 'fake', 'cheat', 'i']\n",
            "now\n",
            "\n",
            "others\n",
            "['want', 'people', 'some', 'wanna', 'teacher']\n",
            "now\n",
            "\n",
            "6\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "262xIQVg7KvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now for part 2\n",
        "def mask_predict_sent(tokenized_sentence):\n",
        "    best_predictions = []\n",
        "    orignal_sentence = tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized_sentence))\n",
        "\n",
        "    min_confidence = 0.80\n",
        "\n",
        "    for i, word in enumerate(tokenized_sentence):\n",
        "        if tokenized_sentence[i] in string.punctuation: #avoid calculating punctuation which will likely have the highest result\n",
        "            continue;\n",
        "\n",
        "        masked_sentence = tokenizer.convert_tokens_to_string(mask(tokenized_sentence, i)).replace(' ##', '')\n",
        "\n",
        "        #Note for later: make sure these partial tokens (##) won't cause any problems\n",
        "\n",
        "        #ranked 1 prediction\n",
        "        prediction = fill_mask(masked_sentence)[0]\n",
        "\n",
        "        #removing special tokens\n",
        "        tokenized_prediction = tokenizer.tokenize(prediction['sequence']) #tokenizing prediction sentence                                      \n",
        "        no_special_tokens_sentence = tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized_prediction), skip_special_tokens=True) #actual sentence\n",
        "\n",
        "        if (orignal_sentence != no_special_tokens_sentence):\n",
        "            if prediction['score'] > min_confidence:\n",
        "                prediction['no_special_token_sentence'] = no_special_tokens_sentence\n",
        "                prediction['index'] = i\n",
        "                best_predictions.append(prediction)\n",
        "\n",
        "                print(word + \"/\" + tokenizer.convert_ids_to_tokens(prediction['token']))\n",
        "\n",
        "    for prediction in best_predictions:\n",
        "        tokenized_sentence[prediction['index']] = tokenizer.convert_ids_to_tokens(prediction['token'])\n",
        "\n",
        "    sentence = ' '.join(tokenized_sentence).replace(' ##', '')\n",
        "\n",
        "    # for ele in sorted_best_predictions:\n",
        "    #     print(ele)\n",
        "\n",
        "    #toke sentence with highest confidence correction without special tokens\n",
        "    return(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4egnYTej-kcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "da6ccc89-1989-4c5a-c430-cb7a04d732d3"
      },
      "source": [
        "#works badly with punctuation and spacing.\n",
        "#maybe an index issue? Like it's replacing the predicted token at the wrong index?\n",
        "#hmmm how should we fix it?\n",
        "\n",
        "sentence = \"I have my own plan too but I do n't same to them , I want to become a Journalist .\"\n",
        "data = {'sentence': sentence, 'correction': '', 'index': []}\n",
        "tokenized_sentence = tokenizer.tokenize(data['sentence'])\n",
        "\n",
        "new_sent = mask_predict_sent(tokenized_sentence)\n",
        "print(new_sent)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "too/,\n",
            "do/'\n",
            "t/the\n",
            "become/be\n",
            "i have my own plan , but i ' n ' the same to them , i want to be a journalist .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcTj8yzkvQW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "818628f4-458e-4d4b-e182-e9806ee8c775"
      },
      "source": [
        "#generate the annotation for the best edit - depracated\n",
        "import errant\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "annotator = errant.load('en', nlp)\n",
        "\n",
        "orig = annotator.parse(orignal_sentence)\n",
        "cor = annotator.parse(highest_confidence_correction['no_special_token_sentence'])\n",
        "edits = annotator.annotate(orig, cor)\n",
        "for e in edits:\n",
        "    #print(e.o_start, str(e.o_end) + \"|||\" + str(e.type) + \"|||\" + e.c_str) #as according to format of data\n",
        "    print(e.to_m2())"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A 3 4|||R:DET|||a|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}